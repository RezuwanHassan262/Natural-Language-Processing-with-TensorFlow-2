{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Sentiment classificatiion(Natural Language Processing with TensorFlow 2).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMRLO9NdJbayZ5Ja5fengo/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Module imports"],"metadata":{"id":"keqcE9flZ8lJ"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"oyTODy0TY8wf","executionInfo":{"status":"ok","timestamp":1648604018998,"user_tz":-360,"elapsed":765,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow.keras import layers\n","from tensorflow import keras"]},{"cell_type":"markdown","source":["Loading and training and testing the data"],"metadata":{"id":"Ocvgs-SsZ-uf"}},{"cell_type":"code","source":["dataset, info = tfds.load('imdb_reviews/subwords8k', with_info = True, as_supervised = True)\n","\n","train_dataset, test_dataset = dataset['train'], dataset['test']\n","encoder = info.features['text'].encoder #Encoder is a reduced dimensional representaton of a set of words\n","\n","print(encoder.subwords[:20])  #Seeing the first 20 words of the dictionary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQuzY8ieZ8Et","executionInfo":{"status":"ok","timestamp":1648604020814,"user_tz":-360,"elapsed":11,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"951d87a8-1941-475c-ad8a-a39643010aa8"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"]},{"output_type":"stream","name":"stdout","text":["['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 'as_']\n"]}]},{"cell_type":"markdown","source":["These reviews are of different lengths and we don't have an identical length through each of the reviews and so when load elements into a matrix, we lead them by doing padding firt.\n","\n","Take the longest reviews and make all the other words of same lengths by adding zeros to the end of each of those words."],"metadata":{"id":"LmKPRtt0dtxL"}},{"cell_type":"code","source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","padded_shapes = ([None],())\n","\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=padded_shapes)\n","test_dataset = test_dataset.padded_batch(BATCH_SIZE, padded_shapes=padded_shapes)"],"metadata":{"id":"Q3k1cbJIbIKP","executionInfo":{"status":"ok","timestamp":1648604023055,"user_tz":-360,"elapsed":10,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["Making the model\n","\n","The model is a sequencial keras model with a bi-directional layer as well as couple of dense layers"],"metadata":{"id":"z5-rjaV9dxlR"}},{"cell_type":"code","source":["model = tf.keras.Sequential([tf.keras.layers.Embedding(encoder.vocab_size,64),\n","                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","                             tf.keras.layers.Dense(64,activation='relu'),\n","                             tf.keras.layers.Dense(1,activation='sigmoid') #probability of a review being positive or negative\n","                             ])  \n","\n","model.compile(optimizer = tf.keras.optimizers.Adam(1e-4), loss = 'binary_crossentropy',metrics = ['accuracy'])"],"metadata":{"id":"smkxWK7cd7lS","executionInfo":{"status":"ok","timestamp":1648604023057,"user_tz":-360,"elapsed":10,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["We are going to train and dump the history of our training object"],"metadata":{"id":"dgKfIPGHhSh4"}},{"cell_type":"code","source":["history = model.fit(train_dataset,epochs=5, validation_data = test_dataset,validation_steps = 30) #epochs=5 | validation_steps = 30"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phIPYiQXhQdL","executionInfo":{"status":"ok","timestamp":1648607977961,"user_tz":-360,"elapsed":3954219,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"791283c1-4d50-4854-9f74-808cd56c5a1d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","391/391 [==============================] - 798s 2s/step - loss: 0.6383 - accuracy: 0.6197 - val_loss: 0.4703 - val_accuracy: 0.8021\n","Epoch 2/5\n","391/391 [==============================] - 754s 2s/step - loss: 0.3500 - accuracy: 0.8596 - val_loss: 0.3448 - val_accuracy: 0.8625\n","Epoch 3/5\n","391/391 [==============================] - 751s 2s/step - loss: 0.2513 - accuracy: 0.9051 - val_loss: 0.3372 - val_accuracy: 0.8708\n","Epoch 4/5\n","391/391 [==============================] - 746s 2s/step - loss: 0.2122 - accuracy: 0.9250 - val_loss: 0.3796 - val_accuracy: 0.8703\n","Epoch 5/5\n","391/391 [==============================] - 756s 2s/step - loss: 0.1863 - accuracy: 0.9344 - val_loss: 0.3281 - val_accuracy: 0.8672\n"]}]},{"cell_type":"markdown","source":["Padding the vectors that we sent despite their sizes"],"metadata":{"id":"H_c1Q4-jkSxu"}},{"cell_type":"code","source":["def pad_to_size(vec, size):\n","  zeros = [0]*(size-len(vec)) #padding with 0's as they don't have any menaing in our dictionary\n","  vec.extend(zeros)\n","  return vec"],"metadata":{"id":"j0CZAUM5hlVt","executionInfo":{"status":"ok","timestamp":1648607977965,"user_tz":-360,"elapsed":30,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["Generate predictions"],"metadata":{"id":"FIWxpV6Gke2v"}},{"cell_type":"code","source":["def sample_predict(sentence, pad):  #We can't use model.predict like before as we have to deal with the PAD issue this time unlike before\n","  encoded_sample_pred_text = encoder.encode(sentence)\n","  if pad:\n","      encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text,64)\n","  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32 )\n","  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text,0))\n","\n","  return predictions\n"],"metadata":{"id":"y9Ii3VoSl3Y0","executionInfo":{"status":"ok","timestamp":1648607977968,"user_tz":-360,"elapsed":28,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Testing the created model"],"metadata":{"id":"d0XgNonsoFUu"}},{"cell_type":"code","source":["#Should return positive\n","sample_text = \"this movie was awesome. The acting was incredible, highly recommended\"\n","predictions = sample_predict(sample_text,pad = True)*100\n","print(\"Probability of this being a positive review\",predictions, '[This is the probability of this statement being a positive review]')\n","\n","#Should return positive\n","sample_text = \"this movie was boring. The acting was mediocre, not recommended\"\n","predictions = sample_predict(sample_text,pad = True)*100\n","print(\"Probability of this being a positive review\",predictions, '[This is the probability of this statement being a positive review]')"],"metadata":{"id":"Jmeqt4oJnSI1","executionInfo":{"status":"ok","timestamp":1648607978603,"user_tz":-360,"elapsed":658,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"043d2ed9-055d-4b88-c77b-9a67f818c1d3"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability of this being a positive review [[85.081184]] [This is the probability of this statement being a positive review]\n","Probability of this being a positive review [[26.737755]] [This is the probability of this statement being a positive review]\n"]}]},{"cell_type":"markdown","source":["#**Making more complex models**"],"metadata":{"id":"Yi1Ogc0q26du"}},{"cell_type":"markdown","source":["New model\n","\n","The model is a sequencial keras model with a bi-directional layer as well as couple of dense layers"],"metadata":{"id":"Ih95ne_Q3ByZ"}},{"cell_type":"code","source":["model = tf.keras.Sequential([tf.keras.layers.Embedding(encoder.vocab_size,64),\n","                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n","                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","                             tf.keras.layers.Dense(64,activation='relu'),\n","                             tf.keras.layers.Dropout(0.5), #Preventing overfitting\n","                             tf.keras.layers.Dense(1,activation='sigmoid') #probability of a review being positive or negative\n","                             ])  \n","\n","model.compile(optimizer = tf.keras.optimizers.Adam(1e-4), loss = 'binary_crossentropy',metrics = ['accuracy'])"],"metadata":{"id":"6iNt0TiK1ifr","executionInfo":{"status":"ok","timestamp":1648607980045,"user_tz":-360,"elapsed":1450,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["\n","We are going to train and dump the history of our training object\n"],"metadata":{"id":"mKghTIBT4E47"}},{"cell_type":"code","source":["history = model.fit(train_dataset,epochs=5, validation_data = test_dataset,validation_steps = 30) #epochs=5 | validation_steps = 30"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2ud0Iwd4Bd9","executionInfo":{"status":"ok","timestamp":1648614974365,"user_tz":-360,"elapsed":6994333,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"500cd296-1fb7-4ad3-e90d-a47b8746e39d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","391/391 [==============================] - 1405s 4s/step - loss: 0.6862 - accuracy: 0.5349 - val_loss: 0.6111 - val_accuracy: 0.6594\n","Epoch 2/5\n","391/391 [==============================] - 1354s 3s/step - loss: 0.4892 - accuracy: 0.7856 - val_loss: 0.3817 - val_accuracy: 0.8484\n","Epoch 3/5\n","391/391 [==============================] - 1391s 4s/step - loss: 0.3099 - accuracy: 0.8853 - val_loss: 0.3320 - val_accuracy: 0.8698\n","Epoch 4/5\n","391/391 [==============================] - 1379s 4s/step - loss: 0.2400 - accuracy: 0.9176 - val_loss: 0.3432 - val_accuracy: 0.8604\n","Epoch 5/5\n","391/391 [==============================] - 1381s 4s/step - loss: 0.2015 - accuracy: 0.9334 - val_loss: 0.3893 - val_accuracy: 0.8677\n"]}]},{"cell_type":"markdown","source":["Padding the vectors that we sent despite their sizes\n"],"metadata":{"id":"JNMacs434XKE"}},{"cell_type":"code","source":["def pad_to_size(vec, size):\n","  zeros = [0]*(size-len(vec)) #padding with 0's as they don't have any menaing in our dictionary\n","  vec.extend(zeros)\n","  return vec"],"metadata":{"id":"NDNks4zZ4V1j","executionInfo":{"status":"ok","timestamp":1648614974368,"user_tz":-360,"elapsed":20,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","Generate predictions"],"metadata":{"id":"JsCStY_74Xo0"}},{"cell_type":"code","source":["def sample_predict(sentence, pad,model_):  #We can't use model.predict like before as we have to deal with the PAD issue this time unlike before\n","  encoded_sample_pred_text = encoder.encode(sentence)\n","  if pad:\n","      encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text,64)\n","  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32 )\n","  predictions = model_.predict(tf.expand_dims(encoded_sample_pred_text,0))\n","\n","  return predictions\n"],"metadata":{"id":"9XfyXx2m4bcJ","executionInfo":{"status":"ok","timestamp":1648614974371,"user_tz":-360,"elapsed":21,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["Testing the created model"],"metadata":{"id":"r9fBw6s04jvj"}},{"cell_type":"code","source":["#Should return positive\n","sample_text = \"this movie was awesome. The acting was incredible, highly recommended\"\n","predictions = sample_predict(sample_text,True, model)*100\n","print(\"Probability of this being a positive review\",predictions, '[This is the probability of this statement being a positive review]')\n","\n","#Should return positive\n","sample_text = \"this movie was boring. The acting was mediocre, not recommended\"\n","predictions = sample_predict(sample_text, True, model)*100\n","print(\"Probability of this being a positive review\",predictions, '[This is the probability of this statement being a positive review]')"],"metadata":{"id":"xG9wOEqS4jbP","executionInfo":{"status":"ok","timestamp":1648614976593,"user_tz":-360,"elapsed":2241,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c00b205-8f9d-474e-a94f-07144cd7da26"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc658209c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc658209c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Probability of this being a positive review [[95.9291]] [This is the probability of this statement being a positive review]\n","Probability of this being a positive review [[11.581442]] [This is the probability of this statement being a positive review]\n"]}]},{"cell_type":"markdown","source":["#epochs=1 | validation_steps = 5"],"metadata":{"id":"PRwIsHk1ViWM"}},{"cell_type":"code","source":["49.734367 < 77.20029 #This is a good sign as the probability of a positive being positive increased \n","33.938503 < 65.097885 #This is a bad sign as the probability of a negative being positive increased\n"],"metadata":{"id":"uNkDzXyg6WO-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#epochs=5 | validation_steps = 30"],"metadata":{"id":"IUURWozh28vW"}},{"cell_type":"code","source":["85.081184 < 95.9291  #This is a good sign as the probability of a positive being positive increased \n","26.737755 > 11.581442 #This is a good sign as the probability of a negative being positive decreased"],"metadata":{"id":"FrmxoTXE2ltc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Much detailed official documentation**\n","\n","https://www.tensorflow.org/text/tutorials/text_classification_rnn\n"],"metadata":{"id":"NNEMBcqsW-MB"}}]}