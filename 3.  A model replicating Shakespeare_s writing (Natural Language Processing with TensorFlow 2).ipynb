{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.  A model replicating Shakespeare's writing .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMSZpnIp6BDl0JeI0r0eObw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Importing the necessary modules and files"],"metadata":{"id":"fF4vBotJXZUn"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"_ELDb4cLWP1l","executionInfo":{"status":"ok","timestamp":1648855443497,"user_tz":-360,"elapsed":414,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'http://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","\n","text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n","#print('Text length:',len(text))\n","#print(text[:250])"]},{"cell_type":"markdown","source":["Sorting the vocabs"],"metadata":{"id":"FQfB1ZvRcDcR"}},{"cell_type":"code","source":["vocabs = sorted(set(text))\n","print(\"Unique characters:\",len(vocabs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXJ487DPW0fp","executionInfo":{"status":"ok","timestamp":1648855449152,"user_tz":-360,"elapsed":9,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"d5e60eab-6ce7-403e-d940-b767971f824f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique characters: 65\n"]}]},{"cell_type":"markdown","source":["##**Step - 1**\n","\n","#Character space to integar representation\n","\n","We are going to assign all the unique chracters in to integars and will map a sentence to vectors based on those assigned integars. "],"metadata":{"id":"FV5k1-T2eH6o"}},{"cell_type":"code","source":["char2idx = {unique:idx for idx,unique in enumerate(vocabs)}\n","idx2char = np.array(vocabs)\n","\n","text_as_int = np.array([char2idx[char] for char in text])\n","\n","print(\"{\")\n","for char, _ in zip(char2idx, range(25)):\n","  print('   {:4s}: {:3d}'.format(repr(char),char2idx[char]))\n","\n","\n","print('--------------')\n","print()\n","print(\"{} characters mapped to int ----> {}\".format(repr(text[:25]),text_as_int[:25]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuULAg-8bmoU","executionInfo":{"status":"ok","timestamp":1648855449971,"user_tz":-360,"elapsed":46,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"6db7a7f0-93d1-40ef-84a7-e2de587189b0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","   '\\n':   0\n","   ' ' :   1\n","   '!' :   2\n","   '$' :   3\n","   '&' :   4\n","   \"'\" :   5\n","   ',' :   6\n","   '-' :   7\n","   '.' :   8\n","   '3' :   9\n","   ':' :  10\n","   ';' :  11\n","   '?' :  12\n","   'A' :  13\n","   'B' :  14\n","   'C' :  15\n","   'D' :  16\n","   'E' :  17\n","   'F' :  18\n","   'G' :  19\n","   'H' :  20\n","   'I' :  21\n","   'J' :  22\n","   'K' :  23\n","   'L' :  24\n","--------------\n","\n","'First Citizen:\\nBefore we ' characters mapped to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n","  1]\n"]}]},{"cell_type":"markdown","source":["##**Step - 2**\n","\n","#Handing the prediction problem\n","\n","The goal here is to feed the model some string of text and then it outputs the most likely characters it thinks will follow based on what it reads in the Spakespearn work\n","\n","We chunk up our data in a sequence of length of 100 and then fo ahead and use that to creae a data set and from there we can create batches of data. In other words, Chunk of sentences or chunks of whatever sequence like the character we want."],"metadata":{"id":"fzxOKHXieOZG"}},{"cell_type":"code","source":["char2idx = {unique:idx for idx,unique in enumerate(vocabs)}\n","idx2char = np.array(vocabs)\n","\n","text_as_int = np.array([char2idx[char] for char in text])\n","\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1) #(seq_length+1) because we're going to feed it a character and try to predict the rest of the characters\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","#for i in char_dataset.take(5):    #priting the first five characters of the dataset\n","#  print(idx2char[i.numpy()])     \n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_plv3gUDbmlK","executionInfo":{"status":"ok","timestamp":1648855449972,"user_tz":-360,"elapsed":34,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"6fdef98a-9aa3-4052-b159-d7efb767ac4b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"]}]},{"cell_type":"markdown","source":["#**Step 3**\n","\n","#Splitting our data into chunk of targets and input text.\n","\n","We have to start with given one character and predict the next set of characters accordigly."],"metadata":{"id":"9TQyTln3qko4"}},{"cell_type":"code","source":["char2idx = {unique:idx for idx,unique in enumerate(vocabs)}\n","idx2char = np.array(vocabs)\n","\n","text_as_int = np.array([char2idx[char] for char in text])\n","\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1) #(seq_length+1) because we're going to feed it a character and try to predict the rest of the characters\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","for input_example, target_example in dataset.take(1):\n","  print(\"Input data\", repr(\"\".join(idx2char[input_example.numpy()])))\n","  print(\"Target data\", repr(\"\".join(idx2char[target_example.numpy()])))\n","\n","print('\\n',\"Note: We just shifted the data one character to the left\",'\\n')\n","\n","for i, (input_idx,target_idx) in enumerate(zip(input_example[:5],target_example[:5])):\n","  print('Step {:4d}'.format(i))\n","  print('   input {} ({:5})'.format(input_idx,repr(idx2char[input_idx])))\n","  print('   expected output {} ({:5})'.format(target_idx,repr(idx2char[target_idx])))\n","\n","print('\\n',\"Note: We are getting the exact character in every step that we expected previously as the input of the next step\",'\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUyrG_E5bmh7","executionInfo":{"status":"ok","timestamp":1648855450584,"user_tz":-360,"elapsed":13,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"6f61f9b0-d5b2-4c51-95e4-6ff48b2aa0bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Input data 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","\n"," Note: We just shifted the data one character to the left \n","\n","Step    0\n","   input 18 ('F'  )\n","   expected output 47 ('i'  )\n","Step    1\n","   input 47 ('i'  )\n","   expected output 56 ('r'  )\n","Step    2\n","   input 56 ('r'  )\n","   expected output 57 ('s'  )\n","Step    3\n","   input 57 ('s'  )\n","   expected output 58 ('t'  )\n","Step    4\n","   input 58 ('t'  )\n","   expected output 1 (' '  )\n","\n"," Note: We are getting the exact character in every step that we expected previously as the input of the next step \n","\n"]}]},{"cell_type":"markdown","source":["#**Step 4**\n","\n","#Creating training batch and training our model"],"metadata":{"id":"dxwxso65yEd8"}},{"cell_type":"code","source":["char2idx = {unique:idx for idx,unique in enumerate(vocabs)}\n","idx2char = np.array(vocabs)\n","\n","text_as_int = np.array([char2idx[char] for char in text])\n","\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1) #(seq_length+1) because we're going to feed it a character and try to predict the rest of the characters\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000 #How many characters to unload\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n","\n","vocab_size = len(vocabs)\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","#Defining the model using a function\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):      #building models\n","  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n","                               tf.keras.layers.GRU(rnn_units, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'),\n","#From integar representation to reduced dimentional representation and model embedding helps the model to find the relationshipn between words. These vectors are all usually orthogonal to one another and no overlapping characters.\n","                               tf.keras.layers.Dense(vocab_size),])\n","  return model\n","\n","#After defining the model, We need to build and compile the model\n","\n","model = build_model(vocab_size = vocab_size,embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size= BATCH_SIZE)\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape,'#(batch_size, seq_length, vocab_size)') #(batch_size, seq_length, vocab_size)\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxe6bEXIbme7","executionInfo":{"status":"ok","timestamp":1648855455997,"user_tz":-360,"elapsed":5423,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"f11f8619-54a1-4183-ca44-a823211e8255"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 65) #(batch_size, seq_length, vocab_size)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (64, None, 256)           16640     \n","                                                                 \n"," gru (GRU)                   (64, None, 1024)          3938304   \n","                                                                 \n"," dense (Dense)               (64, None, 65)            66625     \n","                                                                 \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#Defining, compiling and Training the model"],"metadata":{"id":"Ygh6wovH8kws"}},{"cell_type":"code","source":["char2idx = {unique:idx for idx,unique in enumerate(vocabs)}\n","idx2char = np.array(vocabs)\n","\n","text_as_int = np.array([char2idx[char] for char in text])\n","\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1) #(seq_length+1) because we're going to feed it a character and try to predict the rest of the characters\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000 #How many characters to unload\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n","\n","vocab_size = len(vocabs)\n","embedding_dim = 256\n","rnn_units = 1024\n"],"metadata":{"id":"mq1oAvHWbmb-","executionInfo":{"status":"ok","timestamp":1648855456663,"user_tz":-360,"elapsed":673,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#Defining the model"],"metadata":{"id":"Fh5jLWMAQGk4"}},{"cell_type":"code","source":["#Defining the model using a function\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):      #building models\n","  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n","                               tf.keras.layers.GRU(rnn_units, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'),\n","#From integar representation to reduced dimentional representation and model embedding helps the model to find the relationshipn between words. These vectors are all usually orthogonal to one another and no overlapping characters.\n","                               tf.keras.layers.Dense(vocab_size),])\n","  return model\n","\n","\n","\n","model = build_model(vocab_size = vocab_size,embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size= BATCH_SIZE)"],"metadata":{"id":"IGowHX-zQIuZ","executionInfo":{"status":"ok","timestamp":1648855456664,"user_tz":-360,"elapsed":11,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#Compiling the model\n","\n","After defining the model, We need to build and compile the model"],"metadata":{"id":"9iyWV7s3P51t"}},{"cell_type":"code","source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n","\n","model.compile(optimizer=\"adam\",loss = loss)"],"metadata":{"id":"VmXqUilrbmMK","executionInfo":{"status":"ok","timestamp":1648855456665,"user_tz":-360,"elapsed":11,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#Training the model"],"metadata":{"id":"luTwcxXcQO-C"}},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'chkpt_{epoch}')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only=True)\n","\n","EPOCHS = 1  #epochs=25\n","\n","history = model.fit(dataset,epochs=EPOCHS, callbacks = [checkpoint_callback]) #epochs=25"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkhhploAQOpw","executionInfo":{"status":"ok","timestamp":1648856260224,"user_tz":-360,"elapsed":803569,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"4f838b05-4640-44ac-f11f-d7280e58dd5b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["172/172 [==============================] - 786s 5s/step - loss: 2.7044\n"]}]},{"cell_type":"markdown","source":["#**Final step**\n","\n","#generate the output\n","\n","We wil be using a function to generate the output of the model\n","\n"],"metadata":{"id":"CjGVjc1aRrMW"}},{"cell_type":"markdown","source":["#Compiling the model"],"metadata":{"id":"_O7tRkEqR9Zx"}},{"cell_type":"code","source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n","\n","model.compile(optimizer=\"adam\",loss = loss)"],"metadata":{"id":"PTiBaInuR9Bw","executionInfo":{"status":"ok","timestamp":1648856260228,"user_tz":-360,"elapsed":37,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#Training the model"],"metadata":{"id":"rcTAjaJ8orHV"}},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'chkpt_{epoch}')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only=True)\n","\n","EPOCHS = 25  #epochs=25\n","\n","history = model.fit(dataset,epochs=EPOCHS, callbacks = [checkpoint_callback]) #epochs=25\n","\n","model = build_model(vocab_size = vocab_size,embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size= 1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1,None]))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPmWLPyQRqAl","executionInfo":{"status":"ok","timestamp":1648876186139,"user_tz":-360,"elapsed":19925941,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"26dcfb78-48de-4b1b-9f7e-6e44b32ab905"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","172/172 [==============================] - 789s 5s/step - loss: 1.9603\n","Epoch 2/25\n","172/172 [==============================] - 788s 5s/step - loss: 1.6356\n","Epoch 3/25\n","172/172 [==============================] - 796s 5s/step - loss: 1.4853\n","Epoch 4/25\n","172/172 [==============================] - 792s 5s/step - loss: 1.4003\n","Epoch 5/25\n","172/172 [==============================] - 797s 5s/step - loss: 1.3418\n","Epoch 6/25\n","172/172 [==============================] - 800s 5s/step - loss: 1.2955\n","Epoch 7/25\n","172/172 [==============================] - 800s 5s/step - loss: 1.2536\n","Epoch 8/25\n","172/172 [==============================] - 795s 5s/step - loss: 1.2145\n","Epoch 9/25\n","172/172 [==============================] - 792s 5s/step - loss: 1.1778\n","Epoch 10/25\n","172/172 [==============================] - 794s 5s/step - loss: 1.1381\n","Epoch 11/25\n","172/172 [==============================] - 795s 5s/step - loss: 1.0991\n","Epoch 12/25\n","172/172 [==============================] - 789s 5s/step - loss: 1.0602\n","Epoch 13/25\n","172/172 [==============================] - 794s 5s/step - loss: 1.0221\n","Epoch 14/25\n","172/172 [==============================] - 783s 5s/step - loss: 0.9840\n","Epoch 15/25\n","172/172 [==============================] - 785s 5s/step - loss: 0.9465\n","Epoch 16/25\n","172/172 [==============================] - 791s 5s/step - loss: 0.9109\n","Epoch 17/25\n","172/172 [==============================] - 789s 5s/step - loss: 0.8796\n","Epoch 18/25\n","172/172 [==============================] - 790s 5s/step - loss: 0.8472\n","Epoch 19/25\n","172/172 [==============================] - 788s 5s/step - loss: 0.8218\n","Epoch 20/25\n","172/172 [==============================] - 788s 5s/step - loss: 0.7980\n","Epoch 21/25\n","172/172 [==============================] - 788s 5s/step - loss: 0.7765\n","Epoch 22/25\n","172/172 [==============================] - 788s 5s/step - loss: 0.7573\n","Epoch 23/25\n","172/172 [==============================] - 790s 5s/step - loss: 0.7413\n","Epoch 24/25\n","172/172 [==============================] - 783s 5s/step - loss: 0.7261\n","Epoch 25/25\n","172/172 [==============================] - 783s 5s/step - loss: 0.7128\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (1, None, 256)            16640     \n","                                                                 \n"," gru_2 (GRU)                 (1, None, 1024)           3938304   \n","                                                                 \n"," dense_2 (Dense)             (1, None, 65)             66625     \n","                                                                 \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Prediction problem generate text"],"metadata":{"id":"6GCBQdIEpsEL"}},{"cell_type":"code","source":["def generate_text(model, start_string):\n","  num_generate = 1000\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval,0)\n","\n","  text_generated = []\n","  temp = 1.0  #handles the surpting factor of the text and scales it by a number \n","\n","  model.reset_states()\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    predictions = tf.squeeze(predictions,0)\n","    predictions = predictions/temp\n","    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()      #prediction id of the word predicted by the model\n","    \n","    input_eval = tf.expand_dims([predicted_id],0)\n","    text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string+''.join(text_generated))\n","\n","print(generate_text(model, start_string = 'ROMEO: '))\n"],"metadata":{"id":"LRMvkbrJprhR","executionInfo":{"status":"ok","timestamp":1648876186143,"user_tz":-360,"elapsed":50,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4761a91-5f21-469b-b2f3-02a4bee86425"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO: I\n"]}]},{"cell_type":"markdown","source":["#**Much detailed official documentation**\n","\n","https://www.tensorflow.org/text/tutorials/text_generation"],"metadata":{"id":"XubRFltBXHv0"}},{"cell_type":"code","source":[""],"metadata":{"id":"bYDz1BH2xwy8"},"execution_count":null,"outputs":[]}]}